{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 91844,
          "databundleVersionId": 11361821,
          "isSourceIdPinned": false,
          "sourceType": "competition"
        },
        {
          "sourceId": 11342442,
          "sourceType": "datasetVersion",
          "datasetId": 7096443
        },
        {
          "sourceId": 324634,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": false,
          "modelInstanceId": 273232,
          "modelId": 294172
        },
        {
          "sourceId": 327151,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 274605,
          "modelId": 295497
        },
        {
          "sourceId": 338716,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 283220,
          "modelId": 304082
        },
        {
          "sourceId": 340287,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 284567,
          "modelId": 305407
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee2ff00b6f32469b8780b38bab44ddd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_901cdc2ee8f84384a9ac3bb0a7633636"
            ],
            "layout": "IPY_MODEL_615e99ef89b647cfb8c6817c09fc4f66"
          }
        },
        "6dca77aeb7654cd1a942bb672e9f558b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b84f88e4c60447d481b6de12f195765a",
            "placeholder": "​",
            "style": "IPY_MODEL_e0cc25e3b5c145feb018c0ad44a9dda5",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "9c07fc9d989640ef8a04c2541918eb50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7a7f2cb7b8934dceb472203d5f9f384e",
            "placeholder": "​",
            "style": "IPY_MODEL_18d960a6487f4b01a84627aa83c0ce6b",
            "value": "yashvishe"
          }
        },
        "c8aa4c4701da4326916c2b8e3d26d717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_16aa31e27cb94c9d825d7897353dc93f",
            "placeholder": "​",
            "style": "IPY_MODEL_c7482b26f09348329a2f674524a3bdfc",
            "value": ""
          }
        },
        "cca6c8cb52664daab60aed1217f3d803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_48c6889ce6574ec798961c2495029697",
            "style": "IPY_MODEL_868cd5bdc5f04ad79ce0c1775a8e3d15",
            "tooltip": ""
          }
        },
        "aba876978f3c45aea0d2fa5b60d6335a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f69dbacd1aad4ffb8b1e326797b6516f",
            "placeholder": "​",
            "style": "IPY_MODEL_a2e21364e3104b1fb13b636066a99f56",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "615e99ef89b647cfb8c6817c09fc4f66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b84f88e4c60447d481b6de12f195765a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0cc25e3b5c145feb018c0ad44a9dda5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a7f2cb7b8934dceb472203d5f9f384e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d960a6487f4b01a84627aa83c0ce6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16aa31e27cb94c9d825d7897353dc93f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7482b26f09348329a2f674524a3bdfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48c6889ce6574ec798961c2495029697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868cd5bdc5f04ad79ce0c1775a8e3d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f69dbacd1aad4ffb8b1e326797b6516f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2e21364e3104b1fb13b636066a99f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfc6aaaac1a5406fade2324e1a34ddd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59ed55707d70497796d25c8641e22d5e",
            "placeholder": "​",
            "style": "IPY_MODEL_70bd95462eef413bb47003200c952b4a",
            "value": "Connecting..."
          }
        },
        "59ed55707d70497796d25c8641e22d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70bd95462eef413bb47003200c952b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "901cdc2ee8f84384a9ac3bb0a7633636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8178b9c958bf41adb12df3c3218d2140",
            "placeholder": "​",
            "style": "IPY_MODEL_09489d076f684ba79c0b46a1e674b09b",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "8178b9c958bf41adb12df3c3218d2140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09489d076f684ba79c0b46a1e674b09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashvishe-1327/mlc-sp25-aquafina/blob/bird_clef_yash/Birdclef_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "gYiHTmJrqzR_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "ee2ff00b6f32469b8780b38bab44ddd5",
            "6dca77aeb7654cd1a942bb672e9f558b",
            "9c07fc9d989640ef8a04c2541918eb50",
            "c8aa4c4701da4326916c2b8e3d26d717",
            "cca6c8cb52664daab60aed1217f3d803",
            "aba876978f3c45aea0d2fa5b60d6335a",
            "615e99ef89b647cfb8c6817c09fc4f66",
            "b84f88e4c60447d481b6de12f195765a",
            "e0cc25e3b5c145feb018c0ad44a9dda5",
            "7a7f2cb7b8934dceb472203d5f9f384e",
            "18d960a6487f4b01a84627aa83c0ce6b",
            "16aa31e27cb94c9d825d7897353dc93f",
            "c7482b26f09348329a2f674524a3bdfc",
            "48c6889ce6574ec798961c2495029697",
            "868cd5bdc5f04ad79ce0c1775a8e3d15",
            "f69dbacd1aad4ffb8b1e326797b6516f",
            "a2e21364e3104b1fb13b636066a99f56",
            "bfc6aaaac1a5406fade2324e1a34ddd5",
            "59ed55707d70497796d25c8641e22d5e",
            "70bd95462eef413bb47003200c952b4a",
            "901cdc2ee8f84384a9ac3bb0a7633636",
            "8178b9c958bf41adb12df3c3218d2140",
            "09489d076f684ba79c0b46a1e674b09b"
          ]
        },
        "outputId": "bb83a76c-a791-459a-d531-1164d4273631"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee2ff00b6f32469b8780b38bab44ddd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "source": [
        "birdclef_2025_path = kagglehub.competition_download('birdclef-2025')\n",
        "midcarryhz_efficientnet_b0_birdclef_finetuned_path = kagglehub.dataset_download('midcarryhz/efficientnet-b0-birdclef-finetuned')\n",
        "midcarryhz_efficientnet_b0_test_4_pytorch_default_1_path = kagglehub.model_download('midcarryhz/efficientnet-b0-test_4/PyTorch/default/1')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "k1qeUmSYqzR_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "START = time.time()\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import torchaudio\n",
        "import torchaudio.transforms as AT\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from scipy import signal\n",
        "import gc\n",
        "import warnings\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "torch.set_num_threads(4)\n",
        "\n",
        "TERMINATE_TIME = START + 5300\n",
        "\n",
        "class CFG:\n",
        "\n",
        "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
        "    train_soundscapes = '/kaggle/input/birdclef-2025/train_soundscapes'\n",
        "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
        "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
        "\n",
        "    model_path_list = [\n",
        "        '/kaggle/input/efficientnet-b0-Test-4_stage/model_fold0.pth',\n",
        "        '/kaggle/input/efficientnet-b0-test-4_stage/pytorch/default/1'\n",
        "    ]\n",
        "    model_path = model_path_list[0]\n",
        "\n",
        "    debug = False\n",
        "    debug_start_num = 0\n",
        "    debug_num = 8\n",
        "\n",
        "    FS = 32000\n",
        "    WINDOW_SIZE = 5\n",
        "\n",
        "    N_FFT = 1024\n",
        "    HOP_LENGTH = 16\n",
        "    N_MELS = 148\n",
        "    FMIN = 20\n",
        "    FMAX = 16000\n",
        "    TARGET_SHAPE = (256, 256)\n",
        "\n",
        "    model_name = 'efficientnet_b0'\n",
        "    in_channels = 1\n",
        "    device = 'cpu'\n",
        "\n",
        "    batch_size = 16\n",
        "    use_tta = False\n",
        "    tta_count = 3\n",
        "    threshold = 0.5\n",
        "\n",
        "    use_specific_folds = False\n",
        "    folds = [0, 1]\n",
        "\n",
        "    apply_smoothing = True\n",
        "\n",
        "    apply_noise_reduction = True\n",
        "    apply_normalization = True\n",
        "    noise_reduction_strength = 0.1\n",
        "\n",
        "    smoothing_window = 5\n",
        "    smoothing_weights = [0.15, 0.2, 0.3, 0.2, 0.15]\n",
        "\n",
        "    clear_cache_frequency = 5\n",
        "\n",
        "    use_spec_augment = True\n",
        "\n",
        "    time_mask_param = 30\n",
        "    freq_mask_param = 20\n",
        "    time_mask_count = 1\n",
        "    freq_mask_count = 1\n",
        "\n",
        "    apply_spec_contrast = True\n",
        "    contrast_factor = 0.15\n",
        "\n",
        "    class_thresholds = None\n",
        "\n",
        "    prediction_blend = [0.7, 0.3]\n",
        "\n",
        "    apply_secondary_smoothing = True\n",
        "    secondary_smoothing_weights = [0.2, 0.6, 0.2]\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# PyTorch鸟类分类模型\n",
        "class BirdCLEFModel(nn.Module):\n",
        "    def __init__(self, cfg, num_classes):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.backbone = timm.create_model(\n",
        "            cfg.model_name,\n",
        "            pretrained=False,\n",
        "            in_chans=cfg.in_channels,\n",
        "            drop_rate=0.0,\n",
        "            drop_path_rate=0.0\n",
        "        )\n",
        "\n",
        "        if 'efficientnet' in cfg.model_name:\n",
        "            backbone_out = self.backbone.classifier.in_features\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "        elif 'resnet' in cfg.model_name:\n",
        "            backbone_out = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "        else:\n",
        "            backbone_out = self.backbone.get_classifier().in_features\n",
        "            self.backbone.reset_classifier(0, '')\n",
        "\n",
        "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.feat_dim = backbone_out\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(backbone_out, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        if isinstance(features, dict):\n",
        "            features = features['features']\n",
        "        if len(features.shape) == 4:\n",
        "            features = self.pooling(features)\n",
        "            features = features.view(features.size(0), -1)\n",
        "\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "print(\"Loading label data...\")\n",
        "primary_labels = pd.read_csv('/kaggle/input/birdclef-2025/sample_submission.csv').columns[1:].to_list()\n",
        "taxonomy = pd.read_csv(CFG.taxonomy_csv)\n",
        "\n",
        "def estimate_class_thresholds():\n",
        "    print(\"Estimating optimal thresholds for each class...\")\n",
        "    default_threshold = CFG.threshold\n",
        "\n",
        "    thresholds = {}\n",
        "\n",
        "    if taxonomy is not None and 'primary_label' in taxonomy.columns:\n",
        "        for species in primary_labels:\n",
        "            species_info = taxonomy[taxonomy['primary_label'] == species]\n",
        "\n",
        "            if not species_info.empty:\n",
        "                family = species_info['family'].iloc[0] if 'family' in species_info.columns else None\n",
        "                genus = species_info['genus'].iloc[0] if 'genus' in species_info.columns else None\n",
        "\n",
        "                if family in ['Trochilidae', 'Tyrannidae']:\n",
        "                    thresholds[species] = default_threshold - 0.05\n",
        "                elif family in ['Thraupidae', 'Parulidae']:\n",
        "                    thresholds[species] = default_threshold + 0.05\n",
        "                else:\n",
        "                    thresholds[species] = default_threshold\n",
        "            else:\n",
        "                thresholds[species] = default_threshold\n",
        "    else:\n",
        "        for species in primary_labels:\n",
        "            thresholds[species] = default_threshold\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def get_audio_files():\n",
        "    if os.path.exists(CFG.test_soundscapes) and len(glob.glob(f'{CFG.test_soundscapes}/*.ogg')) > 0:\n",
        "        CFG.debug = False\n",
        "        audio_dir = CFG.test_soundscapes\n",
        "        audio_paths = sorted(glob.glob(f'{audio_dir}/*.ogg'))\n",
        "    else:\n",
        "        print(\"No test files found. Using train soundscapes for debugging.\")\n",
        "        CFG.debug = True\n",
        "        audio_dir = CFG.train_soundscapes\n",
        "        all_audio_paths = sorted(glob.glob(f'{audio_dir}/*.ogg'))\n",
        "        audio_paths = all_audio_paths[CFG.debug_start_num:CFG.debug_start_num + CFG.debug_num]\n",
        "\n",
        "    file_ids = [os.path.splitext(os.path.basename(path))[0] for path in audio_paths]\n",
        "\n",
        "    print(f'Debug mode: {CFG.debug}')\n",
        "    print(f'Number of soundscapes: {len(audio_paths)}')\n",
        "\n",
        "    return audio_paths, file_ids\n",
        "\n",
        "def find_model_files():\n",
        "    model_files = []\n",
        "    for model_path in CFG.model_path_list:\n",
        "        model_dir = Path(model_path)\n",
        "\n",
        "        if model_dir.is_file() and model_dir.suffix in ['.pth', '.pt', '.bin']:\n",
        "            model_files.append(str(model_dir))\n",
        "        else:\n",
        "            print(f\"Looking for model files in {model_dir}...\")\n",
        "\n",
        "            for ext in ['*.pth', '*.pt', '*.bin']:\n",
        "                found_files = list(model_dir.glob(f'**/{ext}'))\n",
        "                if found_files:\n",
        "                    model_files.extend([str(f) for f in found_files])\n",
        "            if not model_files and model_dir.exists():\n",
        "                all_files = list(model_dir.glob('*'))\n",
        "                for f in all_files:\n",
        "                    if f.is_file():\n",
        "                        print(f\"Found potential model file: {f}\")\n",
        "                        model_files.append(str(f))\n",
        "\n",
        "    if model_files:\n",
        "        print(f\"Found {len(model_files)} model files: {model_files}\")\n",
        "    else:\n",
        "        print(f\"No model files found in {CFG.model_path_list}\")\n",
        "\n",
        "    return model_files\n",
        "\n",
        "def load_models():\n",
        "    models = []\n",
        "    model_files = find_model_files()\n",
        "    if not model_files:\n",
        "        print(f\"Warning: No model files found under {CFG.model_path_list}!\")\n",
        "        return models\n",
        "\n",
        "    print(f\"Found a total of {len(model_files)} model files.\")\n",
        "\n",
        "    if CFG.use_specific_folds:\n",
        "        filtered_files = []\n",
        "        for fold in CFG.folds:\n",
        "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
        "            filtered_files.extend(fold_files)\n",
        "        model_files = filtered_files\n",
        "        print(f\"Using {len(model_files)} model files for the specified folds ({CFG.folds}).\")\n",
        "\n",
        "    for model_path in model_files:\n",
        "        try:\n",
        "            print(f\"Loading model: {model_path}\")\n",
        "            checkpoint = torch.load(model_path, map_location='cpu')\n",
        "            model = BirdCLEFModel(CFG, len(primary_labels))\n",
        "\n",
        "            if isinstance(checkpoint, dict):\n",
        "                try:\n",
        "                    if 'model' in checkpoint:\n",
        "                        model.load_state_dict(checkpoint['model'], strict=False)\n",
        "                        model = model.to(CFG.device)\n",
        "                        model.eval()\n",
        "                        models.append(model)\n",
        "                        print(f\"Successfully loaded model from {model_path} (non-strict mode)\")\n",
        "                    elif 'model_state_dict' in checkpoint:\n",
        "                        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "                        model = model.to(CFG.device)\n",
        "                        model.eval()\n",
        "                        models.append(model)\n",
        "                        print(f\"Successfully loaded model from {model_path} (non-strict mode)\")\n",
        "                    elif 'state_dict' in checkpoint:\n",
        "                        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "                        model = model.to(CFG.device)\n",
        "                        model.eval()\n",
        "                        models.append(model)\n",
        "                        print(f\"Successfully loaded model from {model_path} (non-strict mode)\")\n",
        "                    else:\n",
        "                        print(f\"No standard keys found. Trying non-strict direct load.\")\n",
        "                        model.load_state_dict(checkpoint, strict=False)\n",
        "                        model = model.to(CFG.device)\n",
        "                        model.eval()\n",
        "                        models.append(model)\n",
        "                        print(f\"Successfully loaded model from {model_path} (non-strict direct load)\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with non-strict loading: {e}\")\n",
        "                    try:\n",
        "                        if 'model' in checkpoint:\n",
        "                            new_state_dict = {}\n",
        "                            for k, v in checkpoint['model'].items():\n",
        "                                if k.startswith('module.'):\n",
        "                                    new_state_dict[k[7:]] = v\n",
        "                                else:\n",
        "                                    new_state_dict[k] = v\n",
        "                            model.load_state_dict(new_state_dict, strict=False)\n",
        "                            model = model.to(CFG.device)\n",
        "                            model.eval()\n",
        "                            models.append(model)\n",
        "                            print(f\"Successfully loaded model after DataParallel handling\")\n",
        "                        else:\n",
        "                            print(\"No 'model' key found for DataParallel handling\")\n",
        "                    except Exception as e2:\n",
        "                        print(f\"Error with DataParallel handling: {e2}\")\n",
        "            else:\n",
        "                print(f\"Checkpoint is not a dict. Trying non-strict direct load.\")\n",
        "                try:\n",
        "                    model.load_state_dict(checkpoint, strict=False)\n",
        "                    model = model.to(CFG.device)\n",
        "                    model.eval()\n",
        "                    models.append(model)\n",
        "                    print(f\"Successfully loaded model from {model_path} (non-strict direct load)\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading non-dict checkpoint: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_path}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    return models\n",
        "\n",
        "def reduce_noise(audio_data):\n",
        "    if not CFG.apply_noise_reduction:\n",
        "        return audio_data\n",
        "\n",
        "    window_size = 5\n",
        "    audio_denoised = signal.medfilt(audio_data, window_size)\n",
        "\n",
        "    return (1 - CFG.noise_reduction_strength) * audio_data + CFG.noise_reduction_strength * audio_denoised\n",
        "\n",
        "def normalize_audio(audio_data):\n",
        "    if not CFG.apply_normalization:\n",
        "        return audio_data\n",
        "\n",
        "    audio_data = audio_data - np.mean(audio_data)\n",
        "\n",
        "    max_amplitude = np.max(np.abs(audio_data))\n",
        "    if max_amplitude > 0:\n",
        "        audio_data = audio_data / max_amplitude\n",
        "\n",
        "    return audio_data\n",
        "\n",
        "def audio2melspec(audio_data, cfg=CFG):\n",
        "\n",
        "    if np.isnan(audio_data).any():\n",
        "        mean_signal = np.nanmean(audio_data)\n",
        "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
        "\n",
        "    if len(audio_data) < CFG.FS * CFG.WINDOW_SIZE:\n",
        "        audio_data = np.pad(\n",
        "            audio_data,\n",
        "            (0, CFG.FS * CFG.WINDOW_SIZE - len(audio_data)),\n",
        "            mode='constant'\n",
        "        )\n",
        "\n",
        "    audio_data = reduce_noise(audio_data)\n",
        "    audio_data = normalize_audio(audio_data)\n",
        "\n",
        "    mel_spec = librosa.feature.melspectrogram(\n",
        "        y=audio_data,\n",
        "        sr=cfg.FS,\n",
        "        n_fft=cfg.N_FFT,\n",
        "        hop_length=cfg.HOP_LENGTH,\n",
        "        n_mels=cfg.N_MELS,\n",
        "        fmin=cfg.FMIN,\n",
        "        fmax=cfg.FMAX,\n",
        "        power=2.0\n",
        "    )\n",
        "\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
        "\n",
        "    if CFG.apply_spec_contrast:\n",
        "        mel_spec_norm = enhance_spectrogram_contrast(mel_spec_norm, CFG.contrast_factor)\n",
        "\n",
        "    if mel_spec_norm.shape != cfg.TARGET_SHAPE:\n",
        "        mel_spec_norm = cv2.resize(mel_spec_norm, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    return mel_spec_norm.astype(np.float32)\n",
        "\n",
        "def enhance_spectrogram_contrast(spec, factor=0.15):\n",
        "    mean = np.mean(spec)\n",
        "    enhanced = mean + (spec - mean) * (1 + factor)\n",
        "    return np.clip(enhanced, 0, 1)\n",
        "\n",
        "def apply_spec_augment(spec):\n",
        "    if not CFG.use_spec_augment:\n",
        "        return spec\n",
        "\n",
        "    augmented = spec.copy()\n",
        "\n",
        "    for _ in range(CFG.freq_mask_count):\n",
        "        f = np.random.randint(0, CFG.freq_mask_param)\n",
        "        f0 = np.random.randint(0, augmented.shape[0] - f)\n",
        "        augmented[f0:f0 + f, :] = 0\n",
        "\n",
        "    for _ in range(CFG.time_mask_count):\n",
        "        t = np.random.randint(0, CFG.time_mask_param)\n",
        "        t0 = np.random.randint(0, augmented.shape[1] - t)\n",
        "        augmented[:, t0:t0 + t] = 0\n",
        "\n",
        "    return augmented\n",
        "\n",
        "def apply_tta(spec, tta_idx):\n",
        "    result = spec.copy()\n",
        "\n",
        "    if tta_idx == 0:\n",
        "        return result\n",
        "    elif tta_idx == 1:\n",
        "        return np.flip(result, axis=1).copy()\n",
        "    elif tta_idx == 2:\n",
        "        return np.flip(result, axis=0).copy()\n",
        "    elif tta_idx == 3:\n",
        "        return np.flip(np.flip(result, axis=1), axis=0).copy()\n",
        "    elif tta_idx == 4:\n",
        "        return np.roll(result, shift=3, axis=0)\n",
        "    elif tta_idx == 5:\n",
        "        return np.roll(result, shift=-3, axis=0)\n",
        "    else:\n",
        "        return result\n",
        "\n",
        "def predict_on_audio(audio_path, models):\n",
        "    predictions = []\n",
        "    row_ids = []\n",
        "    soundscape_id = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "\n",
        "    try:\n",
        "        print(f\"Processing {soundscape_id}\")\n",
        "        audio_data, _ = librosa.load(audio_path, sr=CFG.FS)\n",
        "        total_segments = int(len(audio_data) / (CFG.FS * CFG.WINDOW_SIZE))\n",
        "\n",
        "        segment_predictions = []\n",
        "\n",
        "        for segment_idx in range(total_segments):\n",
        "            if time.time() > TERMINATE_TIME:\n",
        "                print(\"Time limit reached, stopping processing early\")\n",
        "                return row_ids, predictions\n",
        "\n",
        "            start_sample = segment_idx * CFG.FS * CFG.WINDOW_SIZE\n",
        "            end_sample = start_sample + CFG.FS * CFG.WINDOW_SIZE\n",
        "            segment_audio = audio_data[start_sample:end_sample]\n",
        "\n",
        "            end_time_sec = (segment_idx + 1) * CFG.WINDOW_SIZE\n",
        "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
        "            row_ids.append(row_id)\n",
        "\n",
        "            try:\n",
        "                if CFG.use_tta:\n",
        "                    all_preds = []\n",
        "                    for tta_idx in range(CFG.tta_count):\n",
        "                        mel_spec = audio2melspec(segment_audio)\n",
        "\n",
        "                        if np.random.random() < 0.5 and CFG.use_spec_augment:\n",
        "                            mel_spec = apply_spec_augment(mel_spec)\n",
        "\n",
        "                        mel_spec = apply_tta(mel_spec, tta_idx)\n",
        "                        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                        mel_spec_tensor = mel_spec_tensor.to(CFG.device)\n",
        "\n",
        "                        segment_preds = []\n",
        "                        for model in models:\n",
        "                            with torch.no_grad():\n",
        "                                outputs = model(mel_spec_tensor)\n",
        "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                                segment_preds.append(probs)\n",
        "\n",
        "                        if len(models) > 1:\n",
        "                            weights = np.linspace(0.8, 1.2, len(models))\n",
        "                            weights = weights / weights.sum()\n",
        "                            avg_preds = np.average(segment_preds, axis=0, weights=weights)\n",
        "                        else:\n",
        "                            avg_preds = segment_preds[0]\n",
        "\n",
        "                        all_preds.append(avg_preds)\n",
        "\n",
        "                    final_preds = np.mean(all_preds, axis=0)\n",
        "                else:\n",
        "                    mel_spec = audio2melspec(segment_audio)\n",
        "                    mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                    mel_spec_tensor = mel_spec_tensor.to(CFG.device)\n",
        "\n",
        "                    segment_preds = []\n",
        "                    for model in models:\n",
        "                        with torch.no_grad():\n",
        "                            outputs = model(mel_spec_tensor)\n",
        "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                            segment_preds.append(probs)\n",
        "\n",
        "                    if len(models) > 1:\n",
        "                        weights = np.linspace(0.8, 1.2, len(models))\n",
        "                        weights = weights / weights.sum()\n",
        "                        final_preds = np.average(segment_preds, axis=0, weights=weights)\n",
        "                    else:\n",
        "                        final_preds = segment_preds[0]\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing segment {segment_idx}: {e}\")\n",
        "                final_preds = np.zeros(len(primary_labels)) if len(predictions) == 0 else predictions[-1]\n",
        "\n",
        "            segment_predictions.append(final_preds)\n",
        "\n",
        "        for i in range(len(segment_predictions)):\n",
        "            if i > 0 and i < len(segment_predictions) - 1:\n",
        "                blended_pred = (\n",
        "                    CFG.prediction_blend[0] * segment_predictions[i] +\n",
        "                    CFG.prediction_blend[1] * 0.5 * (segment_predictions[i-1] + segment_predictions[i+1])\n",
        "                )\n",
        "                predictions.append(blended_pred)\n",
        "            else:\n",
        "                predictions.append(segment_predictions[i])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        row_ids = []\n",
        "        predictions = []\n",
        "\n",
        "    return row_ids, predictions\n",
        "\n",
        "def smooth_predictions(submission_df):\n",
        "    print(\"Smoothing prediction results...\")\n",
        "    sub = submission_df.copy()\n",
        "    cols = sub.columns[1:]\n",
        "    sub['group'] = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
        "    unique_groups = sub['group'].unique()\n",
        "\n",
        "    for group in unique_groups:\n",
        "        group_mask = sub['group'] == group\n",
        "        sub_group = sub[group_mask].copy()\n",
        "        predictions = sub_group[cols].values\n",
        "        new_predictions = predictions.copy()\n",
        "\n",
        "        if predictions.shape[0] > 1:\n",
        "            window = CFG.smoothing_window\n",
        "            weights = CFG.smoothing_weights\n",
        "            half_window = window // 2\n",
        "\n",
        "            for i in range(half_window):\n",
        "                valid_window = i + half_window + 1\n",
        "                valid_weights = weights[-valid_window:]\n",
        "                valid_weights = valid_weights / np.sum(valid_weights)\n",
        "                new_predictions[i] = np.average(predictions[:valid_window], axis=0, weights=valid_weights)\n",
        "\n",
        "                valid_window = i + half_window + 1\n",
        "                valid_weights = weights[:valid_window]\n",
        "                valid_weights = valid_weights / np.sum(valid_weights)\n",
        "                new_predictions[-(i+1)] = np.average(predictions[-valid_window:], axis=0, weights=valid_weights)\n",
        "\n",
        "            for i in range(half_window, predictions.shape[0] - half_window):\n",
        "                window_start = i - half_window\n",
        "                window_end = i + half_window + 1\n",
        "                new_predictions[i] = np.average(predictions[window_start:window_end], axis=0, weights=weights)\n",
        "\n",
        "        sub.loc[group_mask, cols] = new_predictions\n",
        "\n",
        "    sub.drop('group', axis=1, inplace=True)\n",
        "    return sub\n",
        "\n",
        "def apply_thresholds(submission_df, thresholds):\n",
        "    print(\"Applying class-specific thresholds...\")\n",
        "    result_df = submission_df.copy()\n",
        "\n",
        "    for species in primary_labels:\n",
        "        threshold = thresholds.get(species, CFG.threshold)\n",
        "        result_df[species] = (result_df[species] >= threshold).astype(float)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def run_pipeline():\n",
        "    print(f\"Device: {CFG.device}\")\n",
        "    print(f\"TTA enabled: {CFG.use_tta} (variations: {CFG.tta_count if CFG.use_tta else 0})\")\n",
        "\n",
        "    if CFG.class_thresholds is None:\n",
        "        CFG.class_thresholds = estimate_class_thresholds()\n",
        "\n",
        "    models = []\n",
        "    for path_idx, model_path in enumerate(CFG.model_path_list):\n",
        "        print(f\"\\nAttempting to load models from path {path_idx+1}/{len(CFG.model_path_list)}: {model_path}\")\n",
        "        CFG.model_path = model_path\n",
        "        models = load_models()\n",
        "        if models:\n",
        "            print(f\"Successfully loaded {len(models)} models from {model_path}\")\n",
        "            break\n",
        "        else:\n",
        "            print(f\"No models loaded from {model_path}, trying next path...\")\n",
        "\n",
        "    if not models:\n",
        "        print(\"No models found after trying all paths! Please check model paths.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
        "\n",
        "    audio_paths, file_ids = get_audio_files()\n",
        "    print(f\"Found {len(audio_paths)} audio files\")\n",
        "\n",
        "    all_row_ids = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for i, audio_path in enumerate(tqdm(audio_paths)):\n",
        "        row_ids, predictions = predict_on_audio(audio_path, models)\n",
        "\n",
        "        if len(row_ids) > 0 and len(predictions) > 0 and len(row_ids) == len(predictions):\n",
        "            all_row_ids.extend(row_ids)\n",
        "            all_predictions.extend(predictions)\n",
        "        else:\n",
        "            print(f\"Skipping results for {audio_path} due to length mismatch or empty results\")\n",
        "\n",
        "        if (i + 1) % CFG.clear_cache_frequency == 0:\n",
        "            clear_memory()\n",
        "\n",
        "    print(\"Creating submission dataframe...\")\n",
        "    submission_dict = {'row_id': all_row_ids}\n",
        "    for i, species in enumerate(primary_labels):\n",
        "        submission_dict[species] = [pred[i] for pred in all_predictions]\n",
        "\n",
        "    lengths = [len(v) for v in submission_dict.values()]\n",
        "    if len(set(lengths)) > 1:\n",
        "        print(f\"Warning: Inconsistent lengths in submission_dict: {lengths}\")\n",
        "\n",
        "        min_length = min(lengths)\n",
        "        for k in submission_dict:\n",
        "            submission_dict[k] = submission_dict[k][:min_length]\n",
        "\n",
        "    submission_df = pd.DataFrame(submission_dict)\n",
        "    sample_sub = pd.read_csv(CFG.submission_csv)\n",
        "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
        "    if missing_cols:\n",
        "        print(f\"Warning: Missing {len(missing_cols)} columns in submission\")\n",
        "        for col in missing_cols:\n",
        "            submission_df[col] = 0.0\n",
        "\n",
        "    if 'row_id' in sample_sub.columns:\n",
        "        submission_df = submission_df[sample_sub.columns]\n",
        "\n",
        "    if CFG.apply_smoothing:\n",
        "        submission_df = smooth_predictions(submission_df)\n",
        "\n",
        "    if CFG.apply_secondary_smoothing:\n",
        "        cols = submission_df.columns[1:]\n",
        "        groups = submission_df['row_id'].str.rsplit('_', n=1).str[0].values\n",
        "\n",
        "        for group in np.unique(groups):\n",
        "            group_mask = (groups == group)\n",
        "            sub_group = submission_df[group_mask]\n",
        "            predictions = sub_group[cols].values\n",
        "            new_predictions = predictions.copy()\n",
        "\n",
        "            for i in range(1, predictions.shape[0]-1):\n",
        "                new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
        "\n",
        "            if predictions.shape[0] > 1:\n",
        "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
        "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
        "\n",
        "            submission_df.loc[group_mask, cols] = new_predictions\n",
        "\n",
        "    final_df = submission_df\n",
        "\n",
        "    submission_path = 'submission.csv'\n",
        "    final_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Submission saved to {submission_path}\")\n",
        "\n",
        "    print(\"\\nSubmission head:\")\n",
        "    print(final_df.head(5))\n",
        "\n",
        "    print(\"\\nSubmission tail:\")\n",
        "    print(final_df.tail(5))\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def create_mel_transform():\n",
        "    return AT.MelSpectrogram(\n",
        "        sample_rate=CFG.FS,\n",
        "        n_fft=CFG.N_FFT,\n",
        "        win_length=CFG.N_FFT,\n",
        "        hop_length=CFG.HOP_LENGTH,\n",
        "        center=True,\n",
        "        f_min=CFG.FMIN,\n",
        "        f_max=CFG.FMAX,\n",
        "        pad_mode=\"reflect\",\n",
        "        power=2.0,\n",
        "        norm='slaney',\n",
        "        n_mels=CFG.N_MELS,\n",
        "        mel_scale=\"htk\",\n",
        "    )\n",
        "\n",
        "def audio_to_mel_debug(filepath):\n",
        "    waveform, _ = torchaudio.load(filepath, backend=\"soundfile\")\n",
        "    waveform = waveform / torch.max(torch.abs(waveform))\n",
        "    mel_transform = create_mel_transform()\n",
        "    melspec = mel_transform(waveform)\n",
        "    melspec = 10 * torch.log10(melspec + 1e-10)\n",
        "    return melspec\n",
        "\n",
        "def plot_results(results, file_name, audio_dir):\n",
        "    path = os.path.join(audio_dir, file_name + \".ogg\")\n",
        "    specgram = audio_to_mel_debug(path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    axes[0].set_title(file_name)\n",
        "    im = axes[0].imshow(specgram[0], origin=\"lower\", aspect=\"auto\")\n",
        "    axes[0].set_ylabel(\"mel bin\")\n",
        "    axes[0].set_xlabel(\"frame\")\n",
        "    fig.colorbar(im, ax=axes[0])\n",
        "\n",
        "    file_results = results[results[\"row_id\"].str.contains(file_name)]\n",
        "\n",
        "    time_segments = file_results.shape[0]\n",
        "\n",
        "    heatmap = axes[1].pcolor(file_results.iloc[:, 1:].values.T, edgecolors='k',\n",
        "                             linewidths=0.1, vmin=0, vmax=1, cmap='Blues')\n",
        "    fig.colorbar(heatmap, ax=axes[1])\n",
        "\n",
        "    axes[1].set_xticks(np.arange(0, time_segments, 1))\n",
        "    axes[1].set_xticklabels(np.arange(0, time_segments * 5, 5))\n",
        "\n",
        "    axes[1].set_ylabel(\"species\")\n",
        "    axes[1].set_xlabel(\"sec\")\n",
        "\n",
        "    if len(primary_labels) <= 30:\n",
        "        axes[1].set_yticks(np.arange(0.5, len(primary_labels), 1))\n",
        "        axes[1].set_yticklabels(primary_labels)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(f'{file_name}_prediction.png')\n",
        "    plt.close()\n",
        "\n",
        "def check_paths_exist():\n",
        "\n",
        "    for path_name, path in [\n",
        "        (\"test_soundscapes\", CFG.test_soundscapes),\n",
        "        (\"train_soundscapes\", CFG.train_soundscapes),\n",
        "        (\"submission_csv\", CFG.submission_csv),\n",
        "        (\"taxonomy_csv\", CFG.taxonomy_csv)\n",
        "    ]:\n",
        "        exists = os.path.exists(path)\n",
        "        print(f\"{path_name}: {path} - {'exist' if exists else 'not exist'}\")\n",
        "\n",
        "    for i, path in enumerate(CFG.model_path_list):\n",
        "        exists = os.path.exists(path)\n",
        "        print(f\"model_path_list[{i}]: {path} - {'exist' if exists else 'not exist'}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        check_paths_exist()\n",
        "\n",
        "        results = run_pipeline()\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "        if CFG.debug and results is not None:\n",
        "            audio_paths, file_ids = get_audio_files()\n",
        "            audio_dir = CFG.train_soundscapes if CFG.debug else CFG.test_soundscapes\n",
        "\n",
        "            print(\"\\nGenerating visualizations...\")\n",
        "            for file_id in file_ids:\n",
        "                plot_results(results, file_id, audio_dir)\n",
        "                print(f\"Visualization saved for {file_id}\")\n",
        "\n",
        "        print(f\"\\nTotal execution time: {(time.time() - START) / 60:.2f} minutes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T06:18:44.070164Z",
          "iopub.execute_input": "2025-04-15T06:18:44.070521Z",
          "iopub.status.idle": "2025-04-15T06:20:03.513137Z",
          "shell.execute_reply.started": "2025-04-15T06:18:44.070494Z",
          "shell.execute_reply": "2025-04-15T06:20:03.512137Z"
        },
        "id": "_ELXkcn-qzSA"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}